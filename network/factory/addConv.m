function [net, out] = addConv(net, name, input, kx, ky, chin, chout, stride, pad, suffix, varargin)
% Helper function to add a Convolutional + BatchNorm + ReLU
% sequence to the network.
  args.relu = true ;
  args.bias = true ;
  args.dilate = [1 1];
  args.cudnnWorkspaceLimit = 1024*1024*1204 ;
  args.residual = [];
  args.bn = true;
  args = vl_argparse(args, varargin) ;
  if nargin < 10
      suffix = [];
  end
  if args.bias
      pars = {[name '_f'], [name '_b']};
  else
      pars = {[name '_f']};
  end
  net.addLayer([name '_' suffix], ...
               dagnn.Conv('size', [kx ky chin chout], ...
                          'stride', stride, ....
                          'pad', pad, ...
                          'dilate', args.dilate, ...
                          'hasBias', args.bias, ...
                          'opts', {'cudnnworkspacelimit', args.cudnnWorkspaceLimit}), ...
               [input '_' suffix], ...
               [name '_' suffix], ...
               pars) ;
  last_out = [name '_' suffix];
  out = name;
  if args.bn
     net.addLayer([name '_bn_' suffix], ...
               dagnn.BatchNorm('numChannels', chout, 'epsilon', 1e-5), ...
               last_out, [name '_bn_' suffix], ...
               {[name '_bn_w'], [name '_bn_b'], [name '_bn_m']}) ;
     last_out = [name '_bn_' suffix];
     out = [name '_bn'];
  end
  if ~isempty(args.residual)
      net.addLayer([name '_sum_' suffix], dagnn.Sum(), ...
        {last_out, args.residual}, [name '_sum_' suffix]);
      last_out = [name '_sum_' suffix];
      out = [name '_sum'];
  end
  if args.relu
    net.addLayer([name '_relu_' suffix] , ...
                 dagnn.ReLU(), ...
                 last_out, ...
                 [name '_relu_' suffix]) ;
    out = [name '_relu'];
  end
end